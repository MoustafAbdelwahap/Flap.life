# -*- coding: utf-8 -*-
"""FLAP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wTrml6mnukuJSuGbjzpHk9gjhta4XCCe
"""

#pip install transformers[torch]
#pip install accelerate -U
#!pip install streamlit

data = [
    ("“Oh well look at that one,” my uncle leans over and says about my brother-in-law in the living room wearing a dress. “I’d always had my suspicions about him,” he jokes with a disapproving sneer and leans back in his chair, a plate of Southern-style Christmas dinner in his hand.", 1),
    ("With the financial stability that my part-time jobs provided my mother could stay home to raise seven children, my learning-disabled older sister could attend college, my younger sister could go on a mission trip to Korea, and my twin siblings could compete in national math competitions. I’ve seen that even as a high school student, I have so much potential to impact my family and beyond -- how one small act can go a long way.", 0),
    ("Last summer, to continue exploring my interest in engineering, I interned at Boeing. Although I spent long hours researching and working in the lab for the inertial navigation of submarines, I learned most from the little things.", 1),
    ("Success is triumphing over hardships -- willing yourself over anything and everything to achieve the best for yourself and your family. With this scholarship, I will use it to continue focusing on my studies in math and engineering, instead of worrying about making money and sending more back home. It will be an investment into myself for my family.", 1),
    ("Fade in: Two men with thick beards kiss – maybe for once they aren’t wearing colorful flamboyant clothing. Fade in: A woman leaves her house to go to her male best friend’s house and her husband honestly tells her to enjoy herself. Fade in: A college student wanting to study abroad tells his conservative parents the truth…", 1),
    ("As a child of immigrant parents, I learned to take responsibilities for my family and myself at a very young age. Although my parents spoke English, they constantly worked in order to financially support my little brother and I.", 1),
    ("In the future, I hope to pursue my dream of becoming a doctor by attaining an MD, and to double major in Managerial Economics. I intend to study at UC Davis as a Biological Sciences major, where I anticipate to become extremely involved with the student community.", 1),
    ("“If you can’t live off of it, it is useless.” My parents were talking about ice skating: my passion. I started skating as a ten-year-old in Spain, admiring how difficulty and grace intertwine to create beautiful programs, but no one imagined I would still be on the ice seven years and one country later. Even more unimaginable was the thought that ice skating might become one of the most useful parts of my life.", 0),
    ("Through the successes of my efforts, I also realized that poverty was just a societal limitation. I was low-income, not poor. I was still flourishing in school, leading faith-based activities and taking an active role in community service. My low-income status was not a barrier but a launching pad to motivate and propel my success.", 0),
    ("Last summer, to continue exploring my interest in engineering, I interned at Boeing. Although I spent long hours researching and working in the lab for the inertial navigation of submarines, I learned most from the little things. ", 0)   ,
    ("Through the successes of my efforts, I also realized that poverty was just a societal limitation. I was low-income, not poor. I was still flourishing in school, leading faith-based activities and taking an active role in community service. My low-income status was not a barrier but a launching pad to motivate and propel my success. ", 0) ,
    ("nI hope to work in one of their facilities some day. Based on my values, interests, and planned future, I’m applying for the NCS Foundation scholarship because not only will it financially help me, but it can give motivation for me to academically push myself. I hope to use this scholarship in applying for a study abroad program, where I can learn about other cultures’ customs while conducting research there. ", 1) ,
    ("I hope to work in one of their facilities some day. Based on my values, interests, and planned future, I’m applying for the NCS Foundation scholarship because not only will it financially help me, but it can give motivation for me to academically push myself. I hope to use this scholarship in applying for a study abroad program, where I can learn about other cultures’ customs while conducting research there. ", 0)     ,
    ("Although I agree that I will never live off of ice skating, the education and skills I have gained from it have opened countless doors. Ice skating has given me the resilience, work ethic, and inspiration to develop as a teacher and an English speaker. It has improved my academic performance by teaching me rhythm, health, and routine. It also reminds me that a passion does not have to produce money in order for it to hold immense value. Ceramics, for instance, challenges me to experiment with the messy and unexpected. While painting reminds me to be adventurous and patient with my forms of self-expression. I don’t know yet what I will live off of from day to day as I mature; however, the skills my passions have provided me are life-long and irreplaceable. ", 1)
    ]

'''
data = [
   ("This is an accepted essay.", 1) ,
   ("This is an accepted essay.", 0)    ]
'''

# Extract labels from the data
labels = [label for text, label in data]

# Split data into train, validation, and test sets using stratified sampling
from sklearn.model_selection import train_test_split
train_data, temp_data, train_labels, temp_labels = train_test_split(data, labels, test_size=0.3, random_state=42, stratify=labels)
validation_data, test_data, validation_labels, test_labels = train_test_split(temp_data, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels)

# Preprocessing:

import torch
from transformers import BertTokenizer

# Load pre-trained BERT tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Preprocess and tokenize data
def preprocess_data(data):
    preprocessed_data = []

    for text, label in data:
        tokens = tokenizer(text, padding="max_length", truncation=True, return_tensors="pt")
        preprocessed_data.append({"input_ids": tokens["input_ids"], "attention_mask": tokens["attention_mask"], "label": label})

    return preprocessed_data

train_preprocessed = preprocess_data(train_data)
validation_preprocessed = preprocess_data(validation_data)
test_preprocessed = preprocess_data(test_data)

import torch
from transformers import BertForSequenceClassification, BertTokenizer, AdamW
from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler

# Define custom dataset class
class CustomDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

# Define model, optimizer, and hyperparameters
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)
optimizer = AdamW(model.parameters(), lr=2e-5)
batch_size = 8
num_epochs = 5

# Create data loaders
train_dataset = CustomDataset(train_preprocessed)
train_sampler = RandomSampler(train_dataset)
train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)

validation_dataset = CustomDataset(validation_preprocessed)
validation_sampler = SequentialSampler(validation_dataset)
validation_dataloader = DataLoader(validation_dataset, sampler=validation_sampler, batch_size=batch_size)

test_dataset = CustomDataset(test_preprocessed)
test_sampler = SequentialSampler(test_dataset)
test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)

# Training loop
for epoch in range(num_epochs):
    model.train()
    total_train_loss = 0  # To store the total training loss for this epoch

    for batch in train_dataloader:
        optimizer.zero_grad()
        input_ids = batch['input_ids'].squeeze(dim=1)
        attention_mask = batch['attention_mask'].squeeze(dim=1)
        labels = batch['label']
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_train_loss += loss.item()  # Accumulate the batch loss
        loss.backward()
        optimizer.step()

    avg_train_loss = total_train_loss / len(train_dataloader)
    print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}", end=" ")  # Print training loss

    # Validation after each epoch
    model.eval()
    validation_loss = 0
    for batch in validation_dataloader:
        with torch.no_grad():
            input_ids = batch['input_ids'].squeeze(dim=1)
            attention_mask = batch['attention_mask'].squeeze(dim=1)
            labels = batch['label']
            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            validation_loss += outputs.loss.item()

    avg_validation_loss = validation_loss / len(validation_dataloader)
    print(f"Validation Loss: {avg_validation_loss:.4f}")

# Testing
accuracies = []

model.eval()
test_accuracy = 0
with torch.no_grad():
    for batch in test_dataloader:
        input_ids = batch['input_ids'].squeeze(dim=1)
        attention_mask = batch['attention_mask'].squeeze(dim=1)
        labels = batch['label']
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        predictions = logits.argmax(dim=1)
        test_accuracy += (predictions == labels).sum().item()

accuracy = test_accuracy / len(test_dataset)
accuracies.append(accuracy)

# Print progress
print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_validation_loss:.4f}, Test Accuracy: {accuracy:.2%}")

print(f"Test Accuracy: {accuracy:.2%}")

'''
import matplotlib.pyplot as plt

# Plot loss curve
plt.figure(figsize=(10, 6))
plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')
plt.plot(range(1, num_epochs+1), validation_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.grid()
plt.show()

# Plot accuracy curve
plt.figure(figsize=(10, 6))
plt.plot(range(1, num_epochs+1), accuracies, label='Test Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Test Accuracy')
plt.legend()
plt.grid()
plt.show()
'''

#Inference

import torch
from transformers import BertForSequenceClassification, BertTokenizer

# Load trained model
#model = BertForSequenceClassification.from_pretrained("path_to_your_saved_model_directory")
model=model
model.eval()

# Load pre-trained BERT tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Input text
#“Oh well look at that one,” my uncle leans over and says about my brother-in-law in the living room wearing a dress. “I’d always had my suspicions about him,” he jokes with a disapproving sneer and leans back in his chair, a plate of Southern-style Christmas dinner in his hand.
#I was hurt. Why would my own uncle say that like it’s such a terrible thing that my brother-in-law is wearing a dress? That it was the worst thing in the world if my brother-in-law were gay or effeminite.

input_text = " I was hurt. Why would my own uncle say that like it’s such a terrible thing that my brother-in-law is wearing a dress? That it was the worst thing in the world if my brother-in-law were gay or effeminite."
# Preprocess input
tokens = tokenizer(input_text, padding="max_length", truncation=True, return_tensors="pt")
input_ids = tokens["input_ids"]
attention_mask = tokens["attention_mask"]

# Make prediction
with torch.no_grad():
    outputs = model(input_ids, attention_mask=attention_mask)
    logits = outputs.logits
    probabilities = torch.softmax(logits, dim=1)
    predicted_class = torch.argmax(probabilities, dim=1).item()
    predicted_probability = probabilities[0][predicted_class].item()

# Interpret prediction
if predicted_class == 0:
    prediction_label = "Negative/Rejected"
else:
    prediction_label = "Positive/Accepted"

print(f"Predicted Label: {prediction_label}")
print(f"Predicted Probability: {predicted_probability:.4f}")
print(f"The model is approximately: {predicted_probability*100 :.4f}% confident that this essay will be {prediction_label}")

#Saving Model
from transformers import BertForSequenceClassification, BertTokenizer

# Define your trained model
model = model
# Save the model and tokenizer
model.save_pretrained("saved_model_directory")
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
tokenizer.save_pretrained("saved_token_directory")



